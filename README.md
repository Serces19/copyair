# CopyAir - Image-to-Image Translation

## ğŸ“‹ DescripciÃ³n del Proyecto

CopyAir es una aplicaciÃ³n de **Image-to-Image Translation** que utiliza una red U-Net profunda para aplicar transformaciones visuales (como rejuvenecimiento facial) a travÃ©s de los frames de un video. El proyecto transforma un notebook de Jupyter en una arquitectura profesional, escalable y reproducible.

### CaracterÃ­sticas principales:
- âœ… Arquitectura U-Net personalizada con skip connections
- âœ… PÃ©rdida hÃ­brida (L1 + SSIM + Perceptual)
- âœ… Datos organizados con versionado
- âœ… ConfiguraciÃ³n flexible mediante YAML
- âœ… Entrenamiento y validaciÃ³n automatizados
- âœ… Inferencia en video completo
- âœ… Pruebas unitarias incluidas

---

## ğŸ“ Estructura del Proyecto

```
copyair/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 01_raw/              # Videos originales sin procesar
â”‚   â”œâ”€â”€ 02_interim/          # Frames extraÃ­dos, archivos temporales
â”‚   â””â”€â”€ 03_processed/        # Datos listos para entrenamiento
â”‚       â”œâ”€â”€ input/           # ImÃ¡genes de entrada
â”‚       â””â”€â”€ ground_truth/    # ImÃ¡genes de referencia (objetivo)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py       # PairedImageDataset, VideoFrameDataset
â”‚   â”‚   â””â”€â”€ augmentations.py # Transformaciones (albumentations)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ unet.py          # Arquitectura U-Net
â”‚   â”‚   â””â”€â”€ losses.py        # L1, SSIM, Perceptual, HybridLoss
â”‚   â”‚
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ train.py         # Loop de entrenamiento
â”‚       â””â”€â”€ inference.py     # PredicciÃ³n en videos
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ SergioCespedes_TrabajoFinal.ipynb  # Notebook original
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ params.yaml          # HiperparÃ¡metros centralizados
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # Script de entrenamiento
â”‚   â””â”€â”€ predict.py           # Script de inferencia
â”‚
â”œâ”€â”€ models/                  # Checkpoints guardados
â”œâ”€â”€ output_inference/        # Videos/frames generados
â”œâ”€â”€ tests/                   # Pruebas unitarias
â”‚
â”œâ”€â”€ requirements.txt         # Dependencias Python
â”œâ”€â”€ .gitignore              # Archivos ignorados por Git
â””â”€â”€ README.md               # Este archivo
```

---

## ğŸš€ Inicio RÃ¡pido

### 1. InstalaciÃ³n

```bash
# Clonar el repositorio
git clone https://github.com/Serces19/copyair.git
cd copyair

# Crear entorno virtual
python -m venv venv

# Activar entorno (Windows)
venv\Scripts\activate

# Activar entorno (Linux/Mac)
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Iniciar MLFlow
mlflow server --host 0.0.0.0 --port 5000 --allowed-hosts "*"
```

### 2. Preparar Datos

Organiza tus datos en `data/03_processed/`:

```
data/03_processed/
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ frame_1.jpg
â”‚   â”œâ”€â”€ frame_2.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ ground_truth/
    â”œâ”€â”€ frame_1.jpg
    â”œâ”€â”€ frame_2.jpg
    â””â”€â”€ ...
```

### 3. Configurar ParÃ¡metros

Edita `configs/params.yaml`:

```yaml
training:
  epochs: 50
  batch_size: 8
  learning_rate: 0.001
```

### 4. Entrenar Modelo

```bash
python scripts/train.py --config configs/params.yaml --device cuda
```

### 5. Inferencia en Video

```bash
# Aplicar modelo y generar video de salida
# El script detecta automÃ¡ticamente la arquitectura del modelo desde el checkpoint
python scripts/predict.py --model models/best_model.pth --video data/01_raw/input.mp4 --output output3.mp4 --native-resolution
```

**ğŸ¯ DetecciÃ³n AutomÃ¡tica de Arquitectura**: El script `predict.py` ahora detecta automÃ¡ticamente la arquitectura del modelo (UNet, NAFNet, ConvNeXt, etc.) desde los metadatos guardados en el checkpoint. No necesitas especificar la arquitectura manualmente.

#### Convertir Modelos Antiguos

Si tienes un modelo entrenado con la versiÃ³n anterior (sin metadatos), puedes convertirlo:

```bash
# Convertir checkpoint antiguo al nuevo formato con metadatos
python scripts/convert_checkpoint.py --model models/old_model.pth --architecture nafnet --output models/new_model.pth
```

---

## ğŸ“Š Archivos Principales

### `src/data/dataset.py`
- `PairedImageDataset`: Carga pares de imÃ¡genes (entrada/objetivo)
- `VideoFrameDataset`: Carga frames para inferencia

### `src/models/unet.py`
- `ConvBlock`: Bloque convolucional bÃ¡sico
- `DownBlock`: Downsampling con max pooling
- `UpBlock`: Upsampling con skip connections
- `UNet`: Arquitectura completa

### `src/models/losses.py`
- `L1Loss`: PÃ©rdida L1
- `SSIMLoss`: Ãndice de similitud estructural
- `PerceptualLoss`: PÃ©rdida basada en caracterÃ­sticas
- `HybridLoss`: CombinaciÃ³n ponderada

### `src/training/train.py`
- `train_epoch()`: Loop de entrenamiento
- `validate()`: ValidaciÃ³n
- `compute_metrics()`: MÃ©tricas (MSE, PSNR, SSIM)

### `src/training/inference.py`
- `predict_frame()`: PredicciÃ³n de frame individual
- `predict_on_video()`: Procesamiento de video completo
- `extract_frames_from_video()`: Extrae frames

---

## âš™ï¸ ConfiguraciÃ³n (params.yaml)

```yaml
# ParÃ¡metros principales
model:
  base_channels: 64      # Canales iniciales
  
training:
  epochs: 50             # NÃºmero de Ã©pocas
  batch_size: 8          # TamaÃ±o del batch
  learning_rate: 0.001   # Tasa de aprendizaje
  val_split: 0.2         # 20% para validaciÃ³n
  
loss:
  lambda_l1: 0.6         # Peso L1
  lambda_ssim: 0.2       # Peso SSIM
  lambda_perceptual: 0.2 # Peso Perceptual

device: "cuda"           # GPU o CPU
```

---

## ğŸ§ª Pruebas

```bash
# Ejecutar todas las pruebas
pytest tests/ -v

# Pruebas especÃ­ficas
pytest tests/test_dataset.py -v
pytest tests/test_models.py -v
```

---

## ğŸ“ˆ Monitoreo de Experimentos

### Usar MLflow (opcional)

```bash
# Instalar
pip install mlflow

# Habilitar en params.yaml
mlflow:
  enabled: true
  experiment_name: "copyair"

# Ejecutar UI
mlflow ui
```

---

## ğŸ³ ContainerizaciÃ³n (Docker)

```dockerfile
FROM pytorch/pytorch:2.1.0-cuda12.1-runtime-ubuntu22.04

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "scripts/train.py"]
```

Construir y ejecutar:

```bash
docker build -t copyair .
docker run --gpus all copyair
```

---

## ğŸ“ Notas Importantes

1. **Datos**: AsegÃºrate que input/ y ground_truth/ tengan archivos con los mismos nombres
2. **GPU**: Usa `--device cuda` para entrenamientos mÃ¡s rÃ¡pidos
3. **Memoria**: Para imÃ¡genes grandes, reduce `batch_size` en params.yaml
4. **Early Stopping**: Se detiene si no mejora despuÃ©s de N Ã©pocas

---

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Abre un Issue o Pull Request.

---

## ğŸ“„ Licencia

MIT License - Ver LICENSE para detalles

---

## ğŸ‘¤ Autor

**Sergio CÃ©spedes** - Trabajo Final

---

## ğŸ”— Recursos Adicionales

- [PyTorch Documentation](https://pytorch.org/docs/)
- [U-Net Paper](https://arxiv.org/abs/1505.04597)
- [Albumentations](https://albumentations.ai/)

---

**Â¡Feliz entrenamiento! ğŸš€**
