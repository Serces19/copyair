# Learning Rate Schedulers - Gu√≠a de Uso

## Resumen

Se ha implementado un sistema flexible de learning rate schedulers configurables desde `params.yaml`. Ahora puedes elegir entre 7 tipos diferentes de schedulers, incluyendo **constant LR** (sin scheduler).

## Schedulers Disponibles

### 1. **Constant LR** (Recomendado para empezar)
Mantiene el learning rate constante durante todo el entrenamiento.

```yaml
scheduler:
  type: "constant"
```

**Cu√°ndo usar:**
- ‚úÖ Cuando quieres control total sobre el LR
- ‚úÖ Para experimentos iniciales
- ‚úÖ Cuando el modelo ya converge bien sin scheduler

---

### 2. **Cosine Annealing**
Reduce el LR siguiendo una curva coseno suave.

```yaml
scheduler:
  type: "cosine"
  params:
    T_max: 500      # N√∫mero total de √©pocas
    eta_min: 0      # LR m√≠nimo al final
```

**Cu√°ndo usar:**
- ‚úÖ Entrenamiento largo y estable
- ‚úÖ Cuando quieres reducci√≥n suave del LR
- ‚úÖ Est√°ndar en muchos papers

**F√≥rmula:** `lr = eta_min + (lr_inicial - eta_min) * (1 + cos(œÄ * epoch / T_max)) / 2`

---

### 3. **Step LR**
Reduce el LR en pasos discretos cada N √©pocas.

```yaml
scheduler:
  type: "step"
  params:
    step_size: 100  # Reducir cada 100 √©pocas
    gamma: 0.5      # Multiplicar LR por 0.5
```

**Cu√°ndo usar:**
- ‚úÖ Cuando sabes en qu√© √©pocas reducir el LR
- ‚úÖ Para fine-tuning con reducciones agresivas
- ‚úÖ Entrenamiento en etapas

**Ejemplo:** LR=0.001 ‚Üí √©poca 100: 0.0005 ‚Üí √©poca 200: 0.00025

---

### 4. **Exponential LR**
Decaimiento exponencial suave del LR cada √©poca.

```yaml
scheduler:
  type: "exponential"
  params:
    gamma: 0.98  # Multiplicar LR por 0.98 cada √©poca
```

**Cu√°ndo usar:**
- ‚úÖ Reducci√≥n muy gradual del LR
- ‚úÖ Entrenamiento muy largo (>1000 √©pocas)
- ‚úÖ Cuando quieres decay constante

**F√≥rmula:** `lr = lr_inicial * gamma^epoch`

---

### 5. **Reduce on Plateau**
Reduce el LR autom√°ticamente cuando la m√©trica de validaci√≥n se estanca.

```yaml
scheduler:
  type: "plateau"
  params:
    mode: "min"      # Minimizar val_loss
    factor: 0.5      # Reducir LR a la mitad
    patience: 10     # Esperar 10 √©pocas sin mejora
    verbose: true
```

**Cu√°ndo usar:**
- ‚úÖ **MUY RECOMENDADO** para entrenamiento adaptativo
- ‚úÖ Cuando no sabes cu√°ndo reducir el LR
- ‚úÖ Para maximizar convergencia

**‚ö†Ô∏è Nota:** Este scheduler requiere llamar a `scheduler.step(val_loss)` con la m√©trica de validaci√≥n.

---

### 6. **OneCycle LR**
Estrategia de "super-convergence": sube el LR al inicio, luego lo baja.

```yaml
scheduler:
  type: "onecycle"
  params:
    max_lr: 0.01  # LR m√°ximo (opcional, usa learning_rate si no se especifica)
```

**Cu√°ndo usar:**
- ‚úÖ Entrenamiento r√°pido (pocas √©pocas)
- ‚úÖ Para encontrar el mejor LR r√°pidamente
- ‚úÖ Cuando quieres convergencia en <100 √©pocas

**Fases:**
1. Warmup: LR sube de 0 a max_lr (45% del entrenamiento)
2. Annealing: LR baja de max_lr a 0 (55% del entrenamiento)

---

### 7. **Cosine Annealing with Warm Restarts**
Cosine annealing que se "reinicia" peri√≥dicamente.

```yaml
scheduler:
  type: "cosine_warmup"
  params:
    T_0: 50        # Primer restart despu√©s de 50 √©pocas
    T_mult: 2      # Duplicar periodo en cada restart
    eta_min: 0
```

**Cu√°ndo usar:**
- ‚úÖ Para escapar de m√≠nimos locales
- ‚úÖ Entrenamiento muy largo
- ‚úÖ Cuando quieres explorar m√∫ltiples soluciones

**Ejemplo:** Restart en √©pocas 50, 150, 350, 750...

---

## Recomendaciones por Caso de Uso

### üéØ Experimentos Iniciales
```yaml
scheduler:
  type: "constant"
```
Mant√©n el LR constante para entender el comportamiento base.

### üéØ Entrenamiento Est√°ndar (500-1000 √©pocas)
```yaml
scheduler:
  type: "cosine"
  params:
    T_max: 500
    eta_min: 0
```

### üéØ Entrenamiento Adaptativo (Recomendado)
```yaml
scheduler:
  type: "plateau"
  params:
    mode: "min"
    factor: 0.5
    patience: 20
    verbose: true
```

### üéØ Entrenamiento R√°pido (<100 √©pocas)
```yaml
scheduler:
  type: "onecycle"
  params:
    max_lr: 0.01
```

### üéØ Fine-tuning
```yaml
scheduler:
  type: "step"
  params:
    step_size: 50
    gamma: 0.5
```

---

## Implementaci√≥n T√©cnica

### Archivo: `src/training/schedulers.py`
Factory que crea schedulers basado en configuraci√≥n.

### Uso en c√≥digo:
```python
from src.training.schedulers import get_scheduler

scheduler_config = {
    'type': 'constant',
    'params': {}
}
scheduler = get_scheduler(optimizer, scheduler_config)

# En el loop de entrenamiento
for epoch in range(epochs):
    train(...)
    val_loss = validate(...)
    
    # Para la mayor√≠a de schedulers
    scheduler.step()
    
    # Para ReduceLROnPlateau
    # scheduler.step(val_loss)
```

---

## Visualizaci√≥n del LR

Para ver c√≥mo cambia el LR durante el entrenamiento:

```python
current_lr = scheduler.get_last_lr()[0]
print(f"√âpoca {epoch}, LR: {current_lr}")

# Con MLflow (autom√°tico)
mlflow.log_metric('train/lr', current_lr, step=epoch)
```

---

## Comparaci√≥n Visual

```
Constant:     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Cosine:       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤
Step:         ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì        ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì        ‚îó‚îÅ‚îÅ‚îÅ‚îÅ
Exponential:  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
OneCycle:     ‚ï±‚ï±‚ï±‚ï±‚ï±‚ï±‚ï±‚ï±‚ï±‚ï±‚ï±‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤‚ï≤
Plateau:      ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì     ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Cosine+Warm:  ‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤
```

---

## Testing

Para probar un scheduler:

```bash
# Editar configs/params.yaml
# Cambiar scheduler.type a "constant", "cosine", etc.

python scripts/train.py --config configs/params.yaml
```

El LR se loggea autom√°ticamente en MLflow como `train/lr`.

---

## Troubleshooting

### Problema: "Scheduler no soportado"
**Soluci√≥n:** Verifica que el `type` sea uno de: `constant`, `cosine`, `step`, `exponential`, `plateau`, `onecycle`, `cosine_warmup`

### Problema: OneCycleLR no funciona
**Soluci√≥n:** OneCycleLR necesita `steps_per_epoch`. Esto se calcula autom√°ticamente en `train.py`.

### Problema: ReduceLROnPlateau no reduce el LR
**Soluci√≥n:** Aseg√∫rate de llamar `scheduler.step(val_loss)` en lugar de `scheduler.step()`.

---

## Migraci√≥n desde C√≥digo Antiguo

**Antes:**
```python
scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)
```

**Ahora:**
```yaml
# En params.yaml
scheduler:
  type: "cosine"
  params:
    T_max: 500
    eta_min: 0
```

```python
# En train.py
scheduler = get_scheduler(optimizer, config['training']['scheduler'])
```

---

## Referencias

- [PyTorch LR Schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)
- [Super-Convergence (OneCycle)](https://arxiv.org/abs/1708.07120)
- [SGDR (Cosine with Warm Restarts)](https://arxiv.org/abs/1608.03983)
